{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1906-masr.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "JDWEmQlB9y6N"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Da-Capo/Colabstuffs/blob/master/1906_masr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDWEmQlB9y6N",
        "colab_type": "text"
      },
      "source": [
        "# 预处理backup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drblqFolzh8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !wget http://www.openslr.org/resources/33/data_aishell.tgz\n",
        "# !wget http://www.openslr.org/resources/33/resource_aishell.tg\n",
        "# !mv data_aishell.tgz drive/My\\ Drive/MASS_DATA/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xggJZiNC3-2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 转移解压数据 1\n",
        "# !tar -zxvf data_aishell.tgz >/dev/null 2>&1\n",
        "# !tar -zxvf resource_aishell.tgz >/dev/null 2>&1\n",
        "# %cd /content/data_aishell\n",
        "# !for tar in wav/*.tar.gz;  do tar xvf $tar; done >/dev/null 2>&1\n",
        "# %cd /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB9O_Kv585WZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 数据预处理\n",
        "# from pathlib import Path\n",
        "# with open(\"data_aishell/transcript/aishell_transcript_v0.8.txt\") as f:\n",
        "#     label_dict = {}\n",
        "#     for i,line in enumerate(f):\n",
        "#         key,value = line.strip().split(\" \",1)\n",
        "#         label_dict[key] = value.replace(\" \",\"\")\n",
        "# train_f  = open(\"data_aishell/train-sort.manifest\",\"w\")\n",
        "# dev_f    = open(\"data_aishell/dev.manifest\",\"w\")\n",
        "\n",
        "# for f_path in Path(\"/content/data_aishell\").rglob('*.wav'):\n",
        "#     x = str(f_path)\n",
        "#     try:\n",
        "#         y = label_dict[f_path.name.split(\".\")[0]]\n",
        "#         if \"train\" in x:\n",
        "#             train_f.write(x+\",\"+y+\"\\n\")\n",
        "#         if \"test\" in x:\n",
        "#             dev_f.write(x+\",\"+y+\"\\n\")\n",
        "#     except:\n",
        "#         print(x)\n",
        "# train_f.close()\n",
        "# dev_f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjCNpgZh9Cp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 保存数据\n",
        "# !zip -r data_aishell.zip data_aishell >/dev/null 2>&1\n",
        "# !cp data_aishell.zip /content/drive/My\\ Drive/MASS_DATA/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFv_0OJC925x",
        "colab_type": "text"
      },
      "source": [
        "# run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFt446tCZapC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gdown\n",
        "!pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu100/torch_nightly.html\n",
        "!pip install python-levenshtein\n",
        "!pip install tensorboardx\n",
        "!wget https://github.com/libai3/masr/archive/18905979fc091f2256be18a4328805a0b7613704.zip -O masr.zip >/dev/null 2>&1\n",
        "!unzip masr.zip -y >/dev/null 2>&1\n",
        "!mv masr-18905979fc091f2256be18a4328805a0b7613704 masr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOZcD0UFIIqc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 准备数据\n",
        "!gdown https://drive.google.com/uc?id=1FYj9p5OyvZqgJUCGWXTgJMJbqCSx_GBh\n",
        "print(\"解压要挺久的...........\")\n",
        "!unzip data_aishell.zip >/dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKfHF1HUctJI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "cellView": "form",
        "outputId": "efd64e65-38ca-49b4-dc52-7e4b0a9f8e59"
      },
      "source": [
        "#@title data.py\n",
        "%%writefile /content/masr/data.py\n",
        "import torch\n",
        "import librosa\n",
        "import wave\n",
        "import numpy as np\n",
        "import scipy\n",
        "import json\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "sample_rate = 16000\n",
        "window_size = 0.02\n",
        "window_stride = 0.01\n",
        "n_fft = int(sample_rate * window_size)\n",
        "win_length = n_fft\n",
        "hop_length = int(sample_rate * window_stride)\n",
        "window = \"hamming\"\n",
        "\n",
        "\n",
        "def load_audio(wav_path, normalize=True):  # -> numpy array\n",
        "    with wave.open(wav_path) as wav:\n",
        "        wav = np.frombuffer(wav.readframes(wav.getnframes()), dtype=\"int16\")\n",
        "        wav = wav.astype(\"float\")\n",
        "    if normalize:\n",
        "        return (wav - wav.mean()) / wav.std()\n",
        "    else:\n",
        "        return wav\n",
        "\n",
        "\n",
        "def spectrogram(wav, normalize=True):\n",
        "    D = librosa.stft(\n",
        "        wav, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=window\n",
        "    )\n",
        "\n",
        "    spec, phase = librosa.magphase(D)\n",
        "    spec = np.log1p(spec)\n",
        "    spec = torch.FloatTensor(spec)\n",
        "\n",
        "    if normalize:\n",
        "        spec = (spec - spec.mean()) / spec.std()\n",
        "\n",
        "    return spec\n",
        "\n",
        "\n",
        "class MASRDataset(Dataset):\n",
        "    def __init__(self, index_path, labels_path):\n",
        "        with open(index_path) as f:\n",
        "            idx = f.readlines()\n",
        "        idx = [x.strip().split(\",\", 1) for x in idx]\n",
        "        self.idx = idx\n",
        "        with open(labels_path) as f:\n",
        "            labels = json.load(f)\n",
        "        self.labels = dict([(labels[i], i) for i in range(len(labels))])\n",
        "        self.labels_str = labels\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        wav, transcript = self.idx[index]\n",
        "        wav = load_audio(wav)\n",
        "        spect = spectrogram(wav)\n",
        "        transcript = list(filter(None, [self.labels.get(x,1) for x in transcript]))\n",
        "\n",
        "        return spect, transcript\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx)\n",
        "\n",
        "\n",
        "def _collate_fn(batch):\n",
        "    def func(p):\n",
        "        return p[0].size(1)\n",
        "\n",
        "    batch = sorted(batch, key=lambda sample: sample[0].size(1), reverse=True)\n",
        "    longest_sample = max(batch, key=func)[0]\n",
        "    freq_size = longest_sample.size(0)\n",
        "    minibatch_size = len(batch)\n",
        "    max_seqlength = longest_sample.size(1)\n",
        "    inputs = torch.zeros(minibatch_size, freq_size, max_seqlength)\n",
        "    input_lens = torch.IntTensor(minibatch_size)\n",
        "    target_lens = torch.IntTensor(minibatch_size)\n",
        "    targets = []\n",
        "    for x in range(minibatch_size):\n",
        "        sample = batch[x]\n",
        "        tensor = sample[0]\n",
        "        target = sample[1]\n",
        "        seq_length = tensor.size(1)\n",
        "        inputs[x].narrow(1, 0, seq_length).copy_(tensor)\n",
        "        input_lens[x] = seq_length\n",
        "        target_lens[x] = len(target)\n",
        "        targets.extend(target)\n",
        "    targets = torch.IntTensor(targets)\n",
        "    return inputs, targets, input_lens, target_lens\n",
        "\n",
        "\n",
        "class MASRDataLoader(DataLoader):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(MASRDataLoader, self).__init__(*args, **kwargs)\n",
        "        self.collate_fn = _collate_fn"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/masr/data.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlUs4f7zcikq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "cellView": "form",
        "outputId": "3266f179-1191-4607-dfa8-fe5999718023"
      },
      "source": [
        "#@title train.py\n",
        "%%writefile /content/masr/train.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import data\n",
        "from models.conv import GatedConv\n",
        "from tqdm import tqdm\n",
        "from decoder import GreedyDecoder\n",
        "import tensorboardX as tensorboard\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "\n",
        "\n",
        "def train(\n",
        "    model,\n",
        "    epochs=1000,\n",
        "    batch_size=64,\n",
        "    train_index_path=\"/content/data_aishell/train-sort.manifest\",\n",
        "    dev_index_path=\"/content/data_aishell/dev.manifest\",\n",
        "    labels_path=\"/content/data_aishell/labels.json\",\n",
        "    learning_rate=0.6,\n",
        "    momentum=0.8,\n",
        "    max_grad_norm=0.2,\n",
        "    weight_decay=0,\n",
        "):\n",
        "    train_dataset = data.MASRDataset(train_index_path, labels_path)\n",
        "    batchs = (len(train_dataset) + batch_size - 1) // batch_size\n",
        "    dev_dataset = data.MASRDataset(dev_index_path, labels_path)\n",
        "    train_dataloader = data.MASRDataLoader(\n",
        "        train_dataset, batch_size=batch_size, num_workers=8\n",
        "    )\n",
        "    train_dataloader_shuffle = data.MASRDataLoader(\n",
        "        train_dataset, batch_size=batch_size, num_workers=8, shuffle=True\n",
        "    )\n",
        "    dev_dataloader = data.MASRDataLoader(\n",
        "        dev_dataset, batch_size=batch_size, num_workers=8\n",
        "    )\n",
        "    parameters = model.parameters()\n",
        "    optimizer = torch.optim.SGD(\n",
        "        parameters,\n",
        "        lr=learning_rate,\n",
        "        momentum=momentum,\n",
        "        nesterov=True,\n",
        "        weight_decay=weight_decay,\n",
        "    )\n",
        "    ctcloss = nn.CTCLoss()\n",
        "    # lr_sched = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.985)\n",
        "    writer = tensorboard.SummaryWriter()\n",
        "    gstep = 0\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        if epoch > 0:\n",
        "            train_dataloader = train_dataloader_shuffle\n",
        "        # lr_sched.step()\n",
        "        lr = get_lr(optimizer)\n",
        "        writer.add_scalar(\"lr/epoch\", lr, epoch)\n",
        "        for i, (x, y, x_lens, y_lens) in enumerate(train_dataloader):\n",
        "            x = x.to(\"cuda\")\n",
        "            y = y.to(\"cuda\")\n",
        "            out, out_lens = model(x, x_lens)\n",
        "            out = out.transpose(0, 1).transpose(0, 2).log_softmax(2)\n",
        "            loss = ctcloss(out, y, out_lens, y_lens)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            writer.add_scalar(\"loss/step\", loss.item(), gstep)\n",
        "            gstep += 1\n",
        "            print(\n",
        "                \"[{}/{}][{}/{}]\\tLoss = {}\".format(\n",
        "                    epoch + 1, epochs, i, int(batchs), loss.item()\n",
        "                )\n",
        "            )\n",
        "        epoch_loss = epoch_loss / batchs\n",
        "        cer = eval(model, dev_dataloader)\n",
        "        writer.add_scalar(\"loss/epoch\", epoch_loss, epoch)\n",
        "        writer.add_scalar(\"cer/epoch\", cer, epoch)\n",
        "        print(\"Epoch {}: Loss= {}, CER = {}\".format(epoch, epoch_loss, cer))\n",
        "        torch.save(model, \"pretrained/model_{}.pth\".format(epoch))\n",
        "\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group[\"lr\"]\n",
        "\n",
        "\n",
        "def eval(model, dataloader):\n",
        "    model.eval()\n",
        "    decoder = GreedyDecoder(dataloader.dataset.labels_str)\n",
        "    cer = 0\n",
        "    print(\"decoding\")\n",
        "    with torch.no_grad():\n",
        "        for i, (x, y, x_lens, y_lens) in tqdm(enumerate(dataloader)):\n",
        "            x = x.to(\"cuda\")\n",
        "            outs, out_lens = model(x, x_lens)\n",
        "            outs = F.softmax(outs, 1)\n",
        "            outs = outs.transpose(1, 2)\n",
        "            ys = []\n",
        "            offset = 0\n",
        "            for y_len in y_lens:\n",
        "                ys.append(y[offset : offset + y_len])\n",
        "                offset += y_len\n",
        "            out_strings, out_offsets = decoder.decode(outs, out_lens)\n",
        "            y_strings = decoder.convert_to_strings(ys)\n",
        "            for pred, truth in zip(out_strings, y_strings):\n",
        "                trans, ref = pred[0], truth[0]\n",
        "                cer += decoder.cer(trans, ref) / float(len(ref))\n",
        "        cer /= len(dataloader.dataset)\n",
        "    model.train()\n",
        "    return cer\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    with open(\"/content/data_aishell/labels.json\") as f:\n",
        "        vocabulary = json.load(f)\n",
        "        vocabulary = \"\".join(vocabulary)\n",
        "    model = GatedConv(vocabulary)\n",
        "    model.to(\"cuda\")\n",
        "    train(model)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/masr/train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvbpNJ6I-7XE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "56a8b39f-e85b-4233-b623-a887821ed4d0"
      },
      "source": [
        "%cd /content/masr\n",
        "!python train.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/masr\n",
            "[1/1000][0/1877]\tLoss = 130.9761199951172\n",
            "[1/1000][1/1877]\tLoss = 129.25137329101562\n",
            "[1/1000][2/1877]\tLoss = 106.64618682861328\n",
            "[1/1000][3/1877]\tLoss = 24.069087982177734\n",
            "[1/1000][4/1877]\tLoss = 20.969749450683594\n",
            "[1/1000][5/1877]\tLoss = 9.54920768737793\n",
            "[1/1000][6/1877]\tLoss = 44.24606704711914\n",
            "[1/1000][7/1877]\tLoss = 12.4298677444458\n",
            "[1/1000][8/1877]\tLoss = 16.844316482543945\n",
            "[1/1000][9/1877]\tLoss = 8.909416198730469\n",
            "[1/1000][10/1877]\tLoss = 23.530921936035156\n",
            "[1/1000][11/1877]\tLoss = 11.121297836303711\n",
            "[1/1000][12/1877]\tLoss = 12.508390426635742\n",
            "[1/1000][13/1877]\tLoss = 9.294649124145508\n",
            "[1/1000][14/1877]\tLoss = 9.199973106384277\n",
            "[1/1000][15/1877]\tLoss = 9.430288314819336\n",
            "[1/1000][16/1877]\tLoss = 9.182867050170898\n",
            "[1/1000][17/1877]\tLoss = 8.904518127441406\n",
            "[1/1000][18/1877]\tLoss = 8.962848663330078\n",
            "[1/1000][19/1877]\tLoss = 8.982231140136719\n",
            "[1/1000][20/1877]\tLoss = 9.240804672241211\n",
            "[1/1000][21/1877]\tLoss = 9.356718063354492\n",
            "[1/1000][22/1877]\tLoss = 12.035593032836914\n",
            "[1/1000][23/1877]\tLoss = 14.996512413024902\n",
            "[1/1000][24/1877]\tLoss = 29.071582794189453\n",
            "[1/1000][25/1877]\tLoss = 15.310070037841797\n",
            "[1/1000][26/1877]\tLoss = 10.572325706481934\n",
            "[1/1000][27/1877]\tLoss = 11.244497299194336\n",
            "[1/1000][28/1877]\tLoss = 13.112354278564453\n",
            "[1/1000][29/1877]\tLoss = 8.819966316223145\n",
            "[1/1000][30/1877]\tLoss = 11.387353897094727\n",
            "[1/1000][31/1877]\tLoss = 8.864017486572266\n",
            "[1/1000][32/1877]\tLoss = 11.091161727905273\n",
            "[1/1000][33/1877]\tLoss = 8.819273948669434\n",
            "[1/1000][34/1877]\tLoss = 9.069236755371094\n",
            "[1/1000][35/1877]\tLoss = 11.001660346984863\n",
            "[1/1000][36/1877]\tLoss = 8.663589477539062\n",
            "[1/1000][37/1877]\tLoss = 8.666261672973633\n",
            "[1/1000][38/1877]\tLoss = 8.677993774414062\n",
            "[1/1000][39/1877]\tLoss = 8.744071006774902\n",
            "[1/1000][40/1877]\tLoss = 8.827630996704102\n",
            "[1/1000][41/1877]\tLoss = 8.975228309631348\n",
            "[1/1000][42/1877]\tLoss = 8.904842376708984\n",
            "[1/1000][43/1877]\tLoss = 8.914835929870605\n",
            "[1/1000][44/1877]\tLoss = 8.957464218139648\n",
            "[1/1000][45/1877]\tLoss = 8.826667785644531\n",
            "[1/1000][46/1877]\tLoss = 8.930148124694824\n",
            "[1/1000][47/1877]\tLoss = 8.809396743774414\n",
            "[1/1000][48/1877]\tLoss = 8.89453125\n",
            "[1/1000][49/1877]\tLoss = 8.947526931762695\n",
            "[1/1000][50/1877]\tLoss = 9.258626937866211\n",
            "[1/1000][51/1877]\tLoss = 9.5843505859375\n",
            "[1/1000][52/1877]\tLoss = 13.563249588012695\n",
            "[1/1000][53/1877]\tLoss = 12.27635383605957\n",
            "[1/1000][54/1877]\tLoss = 11.393416404724121\n",
            "[1/1000][55/1877]\tLoss = 12.041521072387695\n",
            "[1/1000][56/1877]\tLoss = 11.596567153930664\n",
            "[1/1000][57/1877]\tLoss = 12.147958755493164\n",
            "[1/1000][58/1877]\tLoss = 12.403881072998047\n",
            "[1/1000][59/1877]\tLoss = 12.57130241394043\n",
            "[1/1000][60/1877]\tLoss = 12.030564308166504\n",
            "[1/1000][61/1877]\tLoss = 14.565460205078125\n",
            "[1/1000][62/1877]\tLoss = 12.895845413208008\n",
            "[1/1000][63/1877]\tLoss = 14.164785385131836\n",
            "[1/1000][64/1877]\tLoss = 14.23610782623291\n",
            "[1/1000][65/1877]\tLoss = 15.29542350769043\n",
            "[1/1000][66/1877]\tLoss = 17.148941040039062\n",
            "[1/1000][67/1877]\tLoss = 26.94864273071289\n",
            "[1/1000][68/1877]\tLoss = 24.86817169189453\n",
            "[1/1000][69/1877]\tLoss = 24.727252960205078\n",
            "[1/1000][70/1877]\tLoss = 26.65081024169922\n",
            "[1/1000][71/1877]\tLoss = 26.27935218811035\n",
            "[1/1000][72/1877]\tLoss = 32.9866943359375\n",
            "[1/1000][73/1877]\tLoss = 48.689632415771484\n",
            "[1/1000][74/1877]\tLoss = 57.37677001953125\n",
            "[1/1000][75/1877]\tLoss = 54.11381530761719\n",
            "[1/1000][76/1877]\tLoss = 50.08449172973633\n",
            "[1/1000][77/1877]\tLoss = 53.82727813720703\n",
            "[1/1000][78/1877]\tLoss = 135.26422119140625\n",
            "[1/1000][79/1877]\tLoss = 182.1067352294922\n",
            "[1/1000][80/1877]\tLoss = 352.07867431640625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTuoOqLg-xvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}